{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee29891",
   "metadata": {},
   "source": [
    "# Bring Your Own Converted Documents*\n",
    "\n",
    "In some use cases you will be ingesting documents which have previously been converted or parsed. For example,\n",
    "you might have generated your documents directly in JSON format, or you might have converted the documents\n",
    "with Deep Search, exported, modified and next you would like to re-ingest them.\n",
    "\n",
    "In this example we demonstrate how a folder with converted JSON documents can be uploaded to a data collection\n",
    "in Deep Search.\n",
    "\n",
    "\n",
    "*deprecated; in future we will accept only Docling Docs\n",
    "\n",
    "\n",
    "## Access required\n",
    "\n",
    "The content of this notebook requires access to Deep Search capabilities which are not\n",
    "available on the public access system.\n",
    "\n",
    "[Contact us](https://ds4sd.github.io) if you are interested in exploring\n",
    "these Deep Search capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9a9f1",
   "metadata": {},
   "source": [
    "### Set notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13560ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsnotebooks.settings import ProjectNotebookSettings\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# notebook settings auto-loaded from .env / env vars\n",
    "notebook_settings = ProjectNotebookSettings()\n",
    "\n",
    "PROFILE_NAME = notebook_settings.profile  # profile to use\n",
    "PROJ_KEY = notebook_settings.proj_key  # project to use\n",
    "INDEX_NAME = notebook_settings.new_idx_name  # index to create\n",
    "CLEANUP = notebook_settings.cleanup  # whether to clean up\n",
    "INPUT_FILES_FOLDER = Path(\"../../data/converted/\")\n",
    "TMP_DIR = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531c9c4",
   "metadata": {},
   "source": [
    "### Import example dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea3cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependenices\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# IPython utilities\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Import the deepsearch-toolkit\n",
    "import deepsearch as ds\n",
    "from deepsearch.cps.queries import DataQuery\n",
    "from deepsearch.cps.data_indices import utils as data_indices_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8be733",
   "metadata": {},
   "source": [
    "### Connect to Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae4dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe44f7-89f8-45ec-b6a7-8367d171a1ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18957147",
   "metadata": {},
   "source": [
    "### Create new data index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5496626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data index in your project\n",
    "data_index = api.data_indices.create(proj_key=PROJ_KEY, name=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4649-3e2c-4aad-bd8f-0cbe9195e151",
   "metadata": {},
   "source": [
    "## Upload the data\n",
    "\n",
    "When uploading multiple converted documents in JSON format, we have the option to upload one file at the time, or to package all the documents in a JSONL input.\n",
    "In the [JSONL format](https://jsonlines.org/) each line in the file is a full independent JSON object.\n",
    "\n",
    "In the following code we will read all the JSON file in the input folder and make a single JSONL which is then uploaded to Deep Search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b1c0bc-50f5-4a6f-afec-6860e6de35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input_filename\n",
    "input_dir = Path(TMP_DIR.name)\n",
    "input_filename = input_dir / \"upload.jsonl\"\n",
    "\n",
    "with input_filename.open(\"w\") as f:\n",
    "    for doc_filename in INPUT_FILES_FOLDER.glob(\"*.json\"):\n",
    "        doc = json.load(doc_filename.open())\n",
    "        f.write(json.dumps(doc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c926f1d-16e9-441c-8a92-d2423dd31043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Deep Search upload\n",
    "task = api.data_indices.upload(coords=data_index.source, source=input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d37b279-9e9b-449f-a45f-0217bc6870cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'errors': 0, 'success': 3}, 'name': 'cps-upload'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the task to complete\n",
    "api.tasks.wait_for(PROJ_KEY, task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe83b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The data is now available. You can query it programmatically (see next section) or access it via the Deep Search UI at <br />https://sds.app.accelerate.science//projects/b09ae7561a01dc7c4b0fd21a43bfd93d140766d1/library/private/6e7917be8384cff343e3e51de3e1716423773d66"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        f\"The data is now available. You can query it programmatically (see next section) or access it via the Deep Search UI at <br />{api.client.config.host}/projects/{PROJ_KEY}/library/private/{data_index.source.index_key}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650fffc-566e-46b8-b04b-07a58ff20847",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec4a93",
   "metadata": {},
   "source": [
    "### Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9a31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data index contains 0 entries.\n"
     ]
    }
   ],
   "source": [
    "# Count the documents in the data index\n",
    "query = DataQuery(\"*\", source=[\"\"], limit=0, coordinates=data_index.source)\n",
    "query_results = api.queries.run(query)\n",
    "num_results = query_results.outputs[\"data_count\"]\n",
    "print(f\"The data index contains {num_results} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a669e5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f20c008e7c44548d71a3f27313b5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finished fetching all data. Total is 2 records.\n"
     ]
    }
   ],
   "source": [
    "# Find documents matching query\n",
    "search_query = \"speedup\"\n",
    "query = DataQuery(\n",
    "    search_query,\n",
    "    source=[\"description.title\", \"description.authors\"],\n",
    "    coordinates=data_index.source,\n",
    ")\n",
    "query_results = api.queries.run(query)\n",
    "\n",
    "all_results = []\n",
    "cursor = api.queries.run_paginated_query(query)\n",
    "for result_page in tqdm(cursor):\n",
    "    # Iterate through the results of a single page, and add to the total list\n",
    "    for row in result_page.outputs[\"data_outputs\"]:\n",
    "        print()\n",
    "        # Add row to results table\n",
    "        all_results.append(\n",
    "            {\n",
    "                \"Title\": row[\"_source\"][\"description\"][\"title\"],\n",
    "                \"Authors\": \", \".join(\n",
    "                    [\n",
    "                        author[\"name\"]\n",
    "                        for author in row[\"_source\"][\"description\"].get(\"authors\", [])\n",
    "                    ]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "num_results = len(all_results)\n",
    "print(f\"Finished fetching all data. Total is {num_results} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e997bbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the table with all results\n",
    "df = pd.json_normalize(all_results)\n",
    "display(HTML(df.head().to_html(render_links=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c0c18-54ea-4279-a1c2-c9ff6a9493d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5f9e",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "If enabled, we will delete all the resources created in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0b82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index deleted\n",
      "Temporary directory deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete data index\n",
    "if CLEANUP:\n",
    "    api.data_indices.delete(data_index.source)\n",
    "    print(\"Data index deleted\")\n",
    "    TMP_DIR.cleanup()\n",
    "    print(\"Temporary directory deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
