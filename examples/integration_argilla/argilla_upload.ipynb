{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d83e8e",
   "metadata": {},
   "source": [
    "# Deep Search integrations - Argilla.io\n",
    "\n",
    "In this example we will use the output of the converted document for populating a dataset on\n",
    "[Argilla](https://argilla.io). This enables the user to annotate text for multiple purposes,\n",
    "e.g. text classification, named entities recognition, etc as well as train custom models fitting their purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c90d49",
   "metadata": {},
   "source": [
    "### Setup your environment\n",
    "\n",
    "In this example we require the connection to a running Argilla instance.\n",
    "\n",
    "The [README](./README.md) file of this example describes in more details how to set it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4348828",
   "metadata": {},
   "source": [
    "### Set notebook parameters\n",
    "\n",
    "The following block defines the parameters specific to this example notebook\n",
    "\n",
    "- `INPUT_FILE`: the input PDF to converted and analyzed\n",
    "- `ARGILLA_API_URL`: the API URL of the Argilla instance\n",
    "- `ARGILLA_API_KEY`: the API Key of the Argilla instance\n",
    "- `ARGILLA_DATASET`: the name of the dataset on Argilla\n",
    "- `SPACY_MODEL`: the spaCy model to use for tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsnotebooks.settings import ProjectNotebookSettings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# notebook settings auto-loaded from .env / env vars\n",
    "notebook_settings = ProjectNotebookSettings()\n",
    "\n",
    "PROFILE_NAME = notebook_settings.profile  # the profile to use\n",
    "PROJ_KEY = notebook_settings.proj_key  # the project to use\n",
    "\n",
    "INPUT_FILE = Path(\"../../data/samples/2206.00785.pdf\")\n",
    "\n",
    "# Argilla configuration\n",
    "ARGILLA_API_URL = os.environ[\"ARGILLA_API_URL\"]  # required env var\n",
    "ARGILLA_API_KEY = os.environ[\"ARGILLA_API_KEY\"]  # required env var\n",
    "ARGILLA_DATASET = \"deepsearch-documents\"\n",
    "# Tokenization\n",
    "SPACY_MODEL = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fef9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependenices\n",
    "import json\n",
    "import tempfile\n",
    "import typing\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# IPython utilities\n",
    "from IPython.display import display, Markdown, HTML, display_html\n",
    "\n",
    "# Import the deepsearch-toolkit\n",
    "import deepsearch as ds\n",
    "\n",
    "# Import specific to the example\n",
    "import argilla as rg\n",
    "import spacy\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6caf04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spaCy model\n",
    "!python -m spacy download {SPACY_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c815fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class DocTextSegment(BaseModel):\n",
    "    page: int  # page number\n",
    "    idx: int  # index of text segment in the document\n",
    "    title: Optional[str] = None  # title of the document\n",
    "    name: str  # flavour of text segment\n",
    "    type: str  # type of text segment\n",
    "    text: str  # content of the text segment\n",
    "    text_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of text classification\n",
    "    )\n",
    "    token_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of token classification\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0616c",
   "metadata": {},
   "source": [
    "## Document conversion with Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Deep Search\n",
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the docucment conversion and download the results\n",
    "documents = ds.convert_documents(\n",
    "    api=api, proj_key=PROJ_KEY, source_path=INPUT_FILE, progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded25c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = tempfile.mkdtemp()\n",
    "\n",
    "documents.download_all(result_dir=output_dir, progress_bar=True)\n",
    "\n",
    "converted_docs = {}\n",
    "# group output files and visualize the output\n",
    "for output_file in Path(output_dir).rglob(\"*.json\"):\n",
    "    with open(output_file, 'r') as file:\n",
    "        doc_jsondata = json.loads(file.read())\n",
    "        converted_docs[f\"{output_file}//{output_file.name}\"] = doc_jsondata\n",
    "\n",
    "print(f\"{len(converted_docs)} documents have been loaded after conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a1ed2",
   "metadata": {},
   "source": [
    "## Extract text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_segments = []\n",
    "for doc in converted_docs.values():\n",
    "\n",
    "    doc_title = doc.get(\"description\").get(\"title\")\n",
    "    for idx, text_segment in enumerate(doc[\"main-text\"]):\n",
    "        # filter only components with text\n",
    "        if \"text\" not in text_segment:\n",
    "            continue\n",
    "\n",
    "        # append to the component to the list of segments\n",
    "        text_segments.append(\n",
    "            DocTextSegment(\n",
    "                title=doc_title,\n",
    "                page=text_segment.get(\"prov\", [{}])[0].get(\"page\"),\n",
    "                idx=idx,\n",
    "                name=text_segment.get(\"name\"),\n",
    "                type=text_segment.get(\"type\"),\n",
    "                text=text_segment.get(\"text\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"{len(text_segments)} text segments got extracted from the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c8478",
   "metadata": {},
   "source": [
    "## Log the text segments to Argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Argilla SDK\n",
    "client = rg.Argilla(api_url=ARGILLA_API_URL, api_key=ARGILLA_API_KEY)\n",
    "\n",
    "# Initialize the spaCy NLP model for the tokenization of the text\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for text classification\n",
    "\n",
    "records_text_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    records_text_classificaiton.append(\n",
    "        rg.Record(\n",
    "            fields={\"text\":segment.text},\n",
    "            vectors={},\n",
    "            suggestions=segment.text_classification,\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\", \"idx\", \"title\", \"name\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit text for classification\n",
    "settings = rg.Settings(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\"),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(name=\"text_generation\"),\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(name=\"type\"),\n",
    "        rg.TermsMetadataProperty(name=\"page\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384),\n",
    "    ],\n",
    ")\n",
    "dataset = rg.Dataset(name=f\"{ARGILLA_DATASET}-text\", workspace=\"argilla\", settings=settings)\n",
    "dataset.create()\n",
    "dataset.records.log(records_text_classificaiton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for token classification\n",
    "\n",
    "records_token_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    text = [\" \".join(token.text) for token in nlp(segment.text)]\n",
    "    records_token_classificaiton.append(\n",
    "        rg.Record(\n",
    "            fields={\"text\": \" \".join(str(x) for x in text)},\n",
    "            vectors={},\n",
    "            suggestions=segment.token_classification,\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\", \"idx\", \"title\", \"name\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit tokens for classification\n",
    "# Submit text for classification\n",
    "settings = rg.Settings(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\"),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(name=\"text_generation\"),\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(name=\"type\"),\n",
    "        rg.TermsMetadataProperty(name=\"page\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384),\n",
    "    ],\n",
    ")\n",
    "dataset = rg.Dataset(name=f\"{ARGILLA_DATASET}-token\", workspace=\"argilla\", settings=settings)\n",
    "dataset.create()\n",
    "dataset.records.log(records_token_classificaiton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39de2b5",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now that the documents are converted and uploaded in Argilla, you can use the links printed above to annotate and train your own models.\n",
    "\n",
    "Visit the <a href=\"https://docs.argilla.io\" rel=\"nofollow\" target=\"_blank\">Argilla documentation</a> to learn about its features and check out the <a href=\"https://docs.argilla.io/en/latest/guides/guides.html\" rel=\"nofollow\" target=\"_blank\">Deep Dive Guides</a> and <a href=\"https://docs.argilla.io/en/latest/tutorials/tutorials.html\" rel=\"nofollow\" target=\"_blank\">Tutorials</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
