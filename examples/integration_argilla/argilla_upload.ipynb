{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d83e8e",
   "metadata": {},
   "source": [
    "# Deep Search integrations - Argilla.io\n",
    "\n",
    "In this example we will use the output of the converted document for populating a dataset on\n",
    "[Argilla](https://argilla.io). This enables the user to annotate text for multiple purposes,\n",
    "e.g. text classification, named entities recognition, etc as well as train custom models fitting their purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c90d49",
   "metadata": {},
   "source": [
    "### Setup your environment\n",
    "\n",
    "In this example we require the connection to a running Argilla instance.\n",
    "\n",
    "The [README](./README.md) file of this example describes in more details how to set it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4348828",
   "metadata": {},
   "source": [
    "### Set notebook parameters\n",
    "\n",
    "The following block defines the parameters specific to this example notebook\n",
    "\n",
    "- `INPUT_FILE`: the input PDF to converted and analyzed\n",
    "- `ARGILLA_API_URL`: the API URL of the Argilla instance\n",
    "- `ARGILLA_API_KEY`: the API Key of the Argilla instance\n",
    "- `ARGILLA_DATASET`: the name of the dataset on Argilla\n",
    "- `SPACY_MODEL`: the spaCy model to use for tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bf44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsnotebooks.settings import ProjectNotebookSettings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# notebook settings auto-loaded from .env / env vars\n",
    "notebook_settings = ProjectNotebookSettings()\n",
    "\n",
    "PROFILE_NAME = notebook_settings.profile  # the profile to use\n",
    "PROJ_KEY = notebook_settings.proj_key  # the project to use\n",
    "\n",
    "INPUT_FILE = Path(\"../../data/samples/2206.00785.pdf\")\n",
    "\n",
    "# Argilla configuration\n",
    "ARGILLA_API_URL = os.environ[\"ARGILLA_API_URL\"]  # required env var\n",
    "ARGILLA_API_KEY = os.environ[\"ARGILLA_API_KEY\"]  # required env var\n",
    "ARGILLA_DATASET = \"deepsearch-documents\"\n",
    "# Tokenization\n",
    "SPACY_MODEL = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fef9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependenices\n",
    "import json\n",
    "import tempfile\n",
    "import typing\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# IPython utilities\n",
    "from IPython.display import display, Markdown, HTML, display_html\n",
    "\n",
    "# Import the deepsearch-toolkit\n",
    "import deepsearch as ds\n",
    "\n",
    "# Import specific to the example\n",
    "import argilla as rg\n",
    "import spacy\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6caf04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: setuptools in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.2.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.5.0\n",
      "    Uninstalling en-core-web-sm-3.5.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.5.0\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy model\n",
    "!python -m spacy download {SPACY_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c815fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTextSegment(BaseModel):\n",
    "    page: int  # page number\n",
    "    idx: int  # index of text segment in the document\n",
    "    title: str  # title of the document\n",
    "    name: str  # flavour of text segment\n",
    "    type: str  # type of text segment\n",
    "    text: str  # content of the text segment\n",
    "    text_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of text classification\n",
    "    )\n",
    "    token_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of token classification\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0616c",
   "metadata": {},
   "source": [
    "## Document conversion with Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Deep Search\n",
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88e025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:00<00:00, 214.55it/s]\u001b[38;2;15;98;254m                                                                                                                \u001b[0m\n",
      "Submitting input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:04<00:00,  4.34s/it]\u001b[38;2;15;98;254m                                                                                                                 \u001b[0m\n",
      "Converting input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:17<00:00, 17.77s/it]\u001b[38;2;15;98;254m                                                                                                                 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Launch the docucment conversion and download the results\n",
    "documents = ds.convert_documents(\n",
    "    api=api, proj_key=PROJ_KEY, source_path=INPUT_FILE, progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded25c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading result:   : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  1.46it/s]\u001b[38;2;15;98;254m                                                                                                                 \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 documents have been loaded after conversion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = tempfile.mkdtemp()\n",
    "\n",
    "documents.download_all(result_dir=output_dir, progress_bar=True)\n",
    "\n",
    "converted_docs = {}\n",
    "for output_file in Path(output_dir).rglob(\"json*.zip\"):\n",
    "    with ZipFile(output_file) as archive:\n",
    "        all_files = archive.namelist()\n",
    "        for name in all_files:\n",
    "            if not name.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            doc_jsondata = json.loads(archive.read(name))\n",
    "            converted_docs[f\"{output_file}//{name}\"] = doc_jsondata\n",
    "\n",
    "print(f\"{len(converted_docs)} documents have been loaded after conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a1ed2",
   "metadata": {},
   "source": [
    "## Extract text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "325b3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 text segments got extracted from the document\n"
     ]
    }
   ],
   "source": [
    "text_segments = []\n",
    "for doc in converted_docs.values():\n",
    "\n",
    "    doc_title = doc.get(\"description\").get(\"title\")\n",
    "    for idx, text_segment in enumerate(doc[\"main-text\"]):\n",
    "        # filter only components with text\n",
    "        if \"text\" not in text_segment:\n",
    "            continue\n",
    "\n",
    "        # append to the component to the list of segments\n",
    "        text_segments.append(\n",
    "            DocTextSegment(\n",
    "                title=doc_title,\n",
    "                page=text_segment.get(\"prov\", [{}])[0].get(\"page\"),\n",
    "                idx=idx,\n",
    "                name=text_segment.get(\"name\"),\n",
    "                type=text_segment.get(\"type\"),\n",
    "                text=text_segment.get(\"text\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"{len(text_segments)} text segments got extracted from the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c8478",
   "metadata": {},
   "source": [
    "## Log the text segments to Argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Argilla SDK\n",
    "rg.init(api_url=ARGILLA_API_URL, api_key=ARGILLA_API_KEY)\n",
    "\n",
    "# Initialize the spaCy NLP model for the tokenization of the text\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for text classification\n",
    "\n",
    "records_text_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    records_text_classificaiton.append(\n",
    "        rg.TextClassificationRecord(\n",
    "            text=segment.text,\n",
    "            vectors={},\n",
    "            prediction=segment.text_classification,\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit text for classification\n",
    "rg.log(records_text_classificaiton, name=f\"{ARGILLA_DATASET}-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for token classification\n",
    "\n",
    "records_token_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    records_token_classificaiton.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=segment.text,\n",
    "            tokens=[token.text for token in nlp(segment.text)],\n",
    "            prediction=segment.token_classification,\n",
    "            vectors={},\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit tokens for classification\n",
    "rg.log(records_token_classificaiton, name=f\"{ARGILLA_DATASET}-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39de2b5",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now that the documents are converted and uploaded in Argilla, you can use the links printed above to annotate and train your own models.\n",
    "\n",
    "Visit the <a href=\"https://docs.argilla.io\" rel=\"nofollow\" target=\"_blank\">Argilla documentation</a> to learn about its features and check out the <a href=\"https://docs.argilla.io/en/latest/guides/guides.html\" rel=\"nofollow\" target=\"_blank\">Deep Dive Guides</a> and <a href=\"https://docs.argilla.io/en/latest/tutorials/tutorials.html\" rel=\"nofollow\" target=\"_blank\">Tutorials</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
