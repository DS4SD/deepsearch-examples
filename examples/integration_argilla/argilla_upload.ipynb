{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d83e8e",
   "metadata": {},
   "source": [
    "# Deep Search integrations - Argilla.io\n",
    "\n",
    "In this example we will use the output of the converted document for populating a dataset on\n",
    "[Argilla](https://argilla.io). This enables the user to annotate text for multiple purposes,\n",
    "e.g. text classification, named entities recognition, etc as well as train custom models fitting their purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ebcdd",
   "metadata": {},
   "source": [
    "### Authentication via stored credentials\n",
    "\n",
    "In this example, we initialize the Deep Search client from the credentials\n",
    "contained in the file `../../ds-auth.json`. This can be generated with\n",
    "\n",
    "```shell\n",
    "!deepsearch login --output ../../ds-auth.json\n",
    "```\n",
    "\n",
    "More details in the [docs](https://ds4sd.github.io/deepsearch-toolkit/getting_started/#authentication)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c90d49",
   "metadata": {},
   "source": [
    "### Setup your environment\n",
    "\n",
    "In this example we require the connection to a running Argilla instance.\n",
    "\n",
    "The [README](./README.md) file of this example describes in more details how to set it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4348828",
   "metadata": {},
   "source": [
    "### Notebooks parameters\n",
    "\n",
    "The following block defines the parameters used to execute the notebook\n",
    "\n",
    "- `CONFIG_FILE`: location of the Deep Search configuration file\n",
    "- `INPUT_FILE`: the input PDF to converted and analyzed\n",
    "- `ARGILLA_API_URL`: the API URL of the Argilla instance\n",
    "- `ARGILLA_API_KEY`: the API Key of the Argilla instance\n",
    "- `ARGILLA_DATASET`: the name of the dataset on Argilla\n",
    "- `SPACY_MODEL`: the spaCy model to use for tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bf44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters for the example flow\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE = Path(\"../../ds-auth.json\")\n",
    "PROJ_KEY = \"1234567890abcdefghijklmnopqrstvwyz123456\"\n",
    "\n",
    "INPUT_FILE = Path(\"../../data/samples/2206.00785.pdf\")\n",
    "\n",
    "# Argilla configuration\n",
    "ARGILLA_API_URL = os.getenv(\"ARGILLA_API_URL\")\n",
    "ARGILLA_API_KEY = os.getenv(\"ARGILLA_API_KEY\")\n",
    "ARGILLA_DATASET = \"deepsearch-documents\"\n",
    "# Tokenization\n",
    "SPACY_MODEL = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fef9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependenices\n",
    "import json\n",
    "import tempfile\n",
    "import typing\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# IPython utilities\n",
    "from IPython.display import display, Markdown, HTML, display_html\n",
    "\n",
    "# Import the deepsearch-toolkit\n",
    "import deepsearch as ds\n",
    "\n",
    "# Import specific to the example\n",
    "import argilla as rg\n",
    "import spacy\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6caf04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dol/Library/Caches/pypoetry/virtualenvs/deepsearch-examples-QdWiv-Yx-py3.10/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy model\n",
    "!python -m spacy download {SPACY_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c815fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTextSegment(BaseModel):\n",
    "    page: int  # page number\n",
    "    idx: int  # index of text segment in the document\n",
    "    title: str  # title of the document\n",
    "    name: str  # flavour of text segment\n",
    "    type: str  # type of text segment\n",
    "    text: str  # content of the text segment\n",
    "    text_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of text classification\n",
    "    )\n",
    "    token_classification: typing.Any = (\n",
    "        None  # this could be used to store predictions of token classification\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0616c",
   "metadata": {},
   "source": [
    "## Document conversion with Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Deep Search client from the config file\n",
    "config = ds.DeepSearchConfig.parse_file(CONFIG_FILE)\n",
    "client = ds.CpsApiClient(config)\n",
    "api = ds.CpsApi(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88e025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:00<00:00, 106.49it/s]\u001b[38;2;15;98;254m                                                                               \u001b[0m\n",
      "Submitting input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:21<00:00, 21.52s/it]\u001b[38;2;15;98;254m                                                                                \u001b[0m\n",
      "Converting input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:23<00:00, 23.11s/it]\u001b[38;2;15;98;254m                                                                                \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Launch the docucment conversion and download the results\n",
    "documents = ds.convert_documents(\n",
    "    api=api, proj_key=PROJ_KEY, source_path=INPUT_FILE, progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded25c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading result:   : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:01<00:00,  1.15s/it]\u001b[38;2;15;98;254m                                                                                \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 have been loaded after conversion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = tempfile.mkdtemp()\n",
    "\n",
    "documents.download_all(result_dir=output_dir, progress_bar=True)\n",
    "\n",
    "converted_docs = {}\n",
    "for output_file in Path(output_dir).rglob(\"json*.zip\"):\n",
    "    with ZipFile(output_file) as archive:\n",
    "        all_files = archive.namelist()\n",
    "        for name in all_files:\n",
    "            if not name.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            doc_jsondata = json.loads(archive.read(name))\n",
    "            converted_docs[f\"{output_file}//{name}\"] = doc_jsondata\n",
    "\n",
    "print(f\"{len(converted_docs)} documents have been loaded after conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a1ed2",
   "metadata": {},
   "source": [
    "## Extract text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325b3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 got extracted from the document\n"
     ]
    }
   ],
   "source": [
    "text_segments = []\n",
    "for doc in converted_docs.values():\n",
    "\n",
    "    doc_title = doc.get(\"description\").get(\"title\")\n",
    "    for idx, text_segment in enumerate(doc[\"main-text\"]):\n",
    "        # filter only components with text\n",
    "        if \"text\" not in text_segment:\n",
    "            continue\n",
    "\n",
    "        # append to the component to the list of segments\n",
    "        text_segments.append(\n",
    "            DocTextSegment(\n",
    "                title=doc_title,\n",
    "                page=text_segment.get(\"prov\", [{}])[0].get(\"page\"),\n",
    "                idx=idx,\n",
    "                name=text_segment.get(\"name\"),\n",
    "                type=text_segment.get(\"type\"),\n",
    "                text=text_segment.get(\"text\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"{len(text_segments)} text segments got extracted from the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c8478",
   "metadata": {},
   "source": [
    "## Log the text segments to Argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a40f6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Argilla SDK\n",
    "rg.init(api_url=ARGILLA_API_URL, api_key=ARGILLA_API_KEY)\n",
    "\n",
    "# Initialize the spaCy NLP model for the tokenization of the text\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be9574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for text classification\n",
    "\n",
    "records_text_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    records_text_classificaiton.append(\n",
    "        rg.TextClassificationRecord(\n",
    "            text=segment.text,\n",
    "            vectors={},\n",
    "            prediction=segment.text_classification,\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee2f8ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69ff0d3feef4076a966141b3bfd0c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133</span> records logged to <a href=\"https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-text\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-text</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m133\u001b[0m records logged to \u001b]8;id=303262;https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-text\u001b\\\u001b[4;94mhttps://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-text\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='deepsearch-documents-text', processed=133, failed=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit text for classification\n",
    "rg.log(records_text_classificaiton, name=f\"{ARGILLA_DATASET}-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text segments for token classification\n",
    "\n",
    "records_token_classificaiton = []\n",
    "for segment in text_segments:\n",
    "    records_token_classificaiton.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=segment.text,\n",
    "            tokens=[token.text for token in nlp(segment.text)],\n",
    "            prediction=segment.token_classification,\n",
    "            vectors={},\n",
    "            metadata=segment.dict(\n",
    "                exclude={\"text\", \"text_classification\", \"token_classification\"}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "094e67a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d0269151024a6eb5c6b1d300dfe59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133</span> records logged to <a href=\"https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-token\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-token</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m133\u001b[0m records logged to \u001b]8;id=556751;https://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-token\u001b\\\u001b[4;94mhttps://dolfim-ibm-argilla-test.hf.space/datasets/admin/deepsearch-documents-token\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='deepsearch-documents-token', processed=133, failed=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit tokens for classification\n",
    "rg.log(records_token_classificaiton, name=f\"{ARGILLA_DATASET}-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39de2b5",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now that the documents are converted and uploaded in Argilla, you can use the links printed above to annotate and train your own models.\n",
    "\n",
    "Visit the <a href=\"https://docs.argilla.io\" rel=\"nofollow\" target=\"_blank\">Argilla documentation</a> to learn about its features and check out the <a href=\"https://docs.argilla.io/en/latest/guides/guides.html\" rel=\"nofollow\" target=\"_blank\">Deep Dive Guides</a> and <a href=\"https://docs.argilla.io/en/latest/tutorials/tutorials.html\" rel=\"nofollow\" target=\"_blank\">Tutorials</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
