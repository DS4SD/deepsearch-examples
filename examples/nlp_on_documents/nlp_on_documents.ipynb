{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d96e78",
   "metadata": {},
   "source": [
    "# Document Conversion - Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb626f",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The [Deep Search Toolkit](https://ds4sd.github.io/deepsearch-toolkit/) allows document conversion with the following few lines of code. It's that simple! For more info or step-by-step guide:\n",
    "- Visit https://ds4sd.github.io/deepsearch-toolkit/guide/convert_doc/\n",
    "- Follow this example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9c441",
   "metadata": {},
   "source": [
    "### Set notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsnotebooks.settings import ProjectNotebookSettings\n",
    "\n",
    "# notebook settings auto-loaded from .env / env vars\n",
    "notebook_settings = ProjectNotebookSettings()\n",
    "\n",
    "PROFILE_NAME = notebook_settings.profile  # the profile to use\n",
    "PROJ_KEY = notebook_settings.proj_key  # the project to use\n",
    "\n",
    "# default project_key = 1234567890abcdefghijklmnopqrstvwyz123456"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dc0f1",
   "metadata": {},
   "source": [
    "### Import example dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502cdef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T12:14:25.377422Z",
     "start_time": "2022-08-02T12:14:25.152485Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import deepsearch as ds\n",
    "\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from deepsearch.documents.core.export import export_to_markdown\n",
    "from IPython.display import display, Markdown, HTML, display_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4dcda",
   "metadata": {},
   "source": [
    "### Connect to Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44fbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1200c5-1138-4491-bc33-3b2d5aabe949",
   "metadata": {},
   "source": [
    "## Convert Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec83eb0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T12:14:49.216045Z",
     "start_time": "2022-08-02T12:14:25.380757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting input:     :   0%|\u001b[38;2;15;98;254m                              \u001b[0m| 0/1 [00:00<?, ?it/s]\u001b[38;2;15;98;254m\u001b[0m/home/santana/Documents/dev/deepsearch-examples/venv/lib/python3.10/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `_LiteralGenericAlias` with value `typing.Literal['ApiServer...lsHttpSource', 'object']` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Submitting input:     : 100%|\u001b[38;2;15;98;254m██████████████████████████████\u001b[0m| 1/1 [00:03<00:00,  3.18s/it]\u001b[38;2;15;98;254m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_pages': 11, 'processed_pages': 11, 'truncated_pages': 0}\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"./converted_docs\")\n",
    "\n",
    "fname = \"20140197356.pdf\"\n",
    "\n",
    "documents = ds.convert_documents(\n",
    "    api=api,\n",
    "    proj_key=PROJ_KEY,\n",
    "    source_path=f\"../../data/samples/{fname}\",\n",
    "    progress_bar=True,\n",
    ")\n",
    "documents.download_all(result_dir=output_dir)\n",
    "info = documents.generate_report(result_dir=output_dir)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382c4869-cca9-43fc-8052-c0ab7e9c175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group output files and visualize the output\n",
    "md_files = list(output_dir.glob(\"*.md\"))\n",
    "json_files = list(output_dir.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19f7678-b650-484b-a994-150d0c4ec3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display last document\n",
    "# display(Markdown(md_files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784c8a9-4b96-4385-a04e-40ddbf6c613f",
   "metadata": {},
   "source": [
    "## Analyse Document with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574c77b-58a5-4259-b8e9-03ce7129c3bf",
   "metadata": {},
   "source": [
    "### term counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c995060-aee3-4b6c-ba8b-dce6288e44dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    key  count\n",
      "0                                 METAL      8\n",
      "1                                COPPER      2\n",
      "2                                COBALT      0\n",
      "3                              TUNGSTEN     10\n",
      "4                            MOLYBDENUM      0\n",
      "5                             RUTHENIUM      0\n",
      "6                Self-assembly material      0\n",
      "7         Self-assembly molecular layer      0\n",
      "8                  surface modification      0\n",
      "9                             inhibitor      2\n",
      "10                  corrosion inhibitor      2\n",
      "11                           adsorption      0\n",
      "12                          selectivity      1\n",
      "13                       Anti-corrosion      0\n",
      "14                        contact angle      0\n",
      "15            Area selective deposition      0\n",
      "16  Advanced interconnect metallization      0\n",
      "17                  Integrated circuits      2\n",
      "18              Atomic layer deposition      0\n"
     ]
    }
   ],
   "source": [
    "with open(json_files[-1]) as fr:\n",
    "    doc = json.load(fr)\n",
    "\n",
    "terms = [\n",
    "    \"METAL\",\n",
    "    \"COPPER\",\n",
    "    \"COBALT\",\n",
    "    \"TUNGSTEN\",\n",
    "    \"MOLYBDENUM\",\n",
    "    \"RUTHENIUM\",\n",
    "    \"Self-assembly material\",\n",
    "    \"Self-assembly molecular layer\",\n",
    "    \"surface modification\",\n",
    "    \"inhibitor\",\n",
    "    \"corrosion inhibitor\",\n",
    "    \"adsorption\",\n",
    "    \"selectivity\",\n",
    "    \"Anti-corrosion\",\n",
    "    \"contact angle\",\n",
    "    \"Area selective deposition\",\n",
    "    \"Advanced interconnect metallization\",\n",
    "    \"Integrated circuits\",\n",
    "    \"Atomic layer deposition\",\n",
    "]\n",
    "\n",
    "term_hist = [{\"key\": term, \"count\": 0} for term in terms]\n",
    "\n",
    "for i, item in enumerate(doc[\"main-text\"]):\n",
    "\n",
    "    if \"text\" not in item:\n",
    "        continue\n",
    "\n",
    "    for j, term in enumerate(terms):\n",
    "        term_hist[j][\"count\"] += item[\"text\"].count(term.lower())\n",
    "\n",
    "df = pd.DataFrame(term_hist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3ff4-c444-499e-ae40-53f9833bb5ff",
   "metadata": {},
   "source": [
    "### term analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d12955-dced-4320-b625-225bcb6bcdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsearch_glm.utils.load_pretrained_models import load_pretrained_nlp_models\n",
    "\n",
    "from deepsearch_glm.nlp_utils import (\n",
    "    extract_references_from_doc,\n",
    "    init_nlp_model,\n",
    "    list_nlp_model_configs,\n",
    ")\n",
    "\n",
    "from deepsearch_glm.glm_utils import (\n",
    "    create_glm_config_from_docs,\n",
    "    create_glm_dir,\n",
    "    create_glm_from_docs,\n",
    "    expand_terms,\n",
    "    load_glm,\n",
    "    read_edges_in_dataframe,\n",
    "    read_nodes_in_dataframe,\n",
    "    show_query_result,\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "models = load_pretrained_nlp_models()\n",
    "# print(f\"models: {models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3612b4-bbd2-42d0-ba2d-f8f994565380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                        name    subj_path\n",
      "12    term                             United States c    #/texts/0\n",
      "14    term  Patent Application Publication MOEGGENBORG    #/texts/0\n",
      "19    term                CMP COMPOSITIONS AND METHODS    #/texts/1\n",
      "20    term                            CMP COMPOSITIONS    #/texts/1\n",
      "21    term                                     METHODS    #/texts/1\n",
      "...    ...                                         ...          ...\n",
      "2717  term                   organic or inorganic salt  #/texts/106\n",
      "2718  term                              inorganic salt  #/texts/106\n",
      "2722  term                                 composition  #/texts/107\n",
      "2723  term                                       claim  #/texts/107\n",
      "2725  term   salt additive comprises potassium sulfate  #/texts/107\n",
      "\n",
      "[1521 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(json_files[-1]) as fr:\n",
    "    doc = json.load(fr)\n",
    "\n",
    "model = init_nlp_model(\"language;term\")\n",
    "\n",
    "for i, item in enumerate(doc[\"main-text\"]):\n",
    "\n",
    "    if \"text\" in item:\n",
    "        res = model.apply_on_text(item[\"text\"])\n",
    "        # print(res.keys())\n",
    "\n",
    "        # print(item[\"text\"])\n",
    "        # print(tabulate(res[\"instances\"][\"data\"],\n",
    "        #               headers=res[\"instances\"][\"headers\"]))\n",
    "\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "\n",
    "res = model.apply_on_doc(doc)\n",
    "\n",
    "df = pd.DataFrame(res[\"instances\"][\"data\"], columns=res[\"instances\"][\"headers\"])\n",
    "\n",
    "terms = df[df[\"type\"] == \"term\"][[\"type\", \"name\", \"subj_path\"]]\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665335e2-4a10-44ca-af15-0e4b9f8d031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "odir = \"./glm\"\n",
    "os.makedirs(odir, exist_ok=True)\n",
    "\n",
    "model_names = \"spm;term\"\n",
    "json_files = [str(file) for file in json_files]\n",
    "\n",
    "odir, glm = create_glm_from_docs(odir, json_files, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175550c9-8b1e-4b02-8069-6c36aceafd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "   weight    prob   cumul                   text  count\n",
      "0  0.5625  0.5625  0.5625        CMP composition     27\n",
      "1  0.3750  0.3750  0.9375  polishing composition     18\n",
      "2  0.0625  0.0625  1.0000   abrasive composition      3\n"
     ]
    }
   ],
   "source": [
    "nodes = read_nodes_in_dataframe(\"./glm/nodes.csv\")\n",
    "# print(nodes)\n",
    "\n",
    "# Get all terms of the document\n",
    "if nodes:\n",
    "    terms = nodes[nodes[\"name\"] == \"term\"][[\"total-count\", \"nodes-text\"]]\n",
    "    print(terms)\n",
    "\n",
    "# Get all terms of the document with `composition`\n",
    "res = expand_terms(glm, \"composition\")\n",
    "# show_query_result(res)\n",
    "\n",
    "last_result = res[\"result\"][-1][\"nodes\"]\n",
    "expanded_terms = pd.DataFrame(last_result[\"data\"], columns=last_result[\"headers\"])\n",
    "\n",
    "expanded_terms = expanded_terms[[\"weight\", \"prob\", \"cumul\", \"text\", \"count\"]]\n",
    "print(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01771757-70c3-44cb-824c-1fd9b716a99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
