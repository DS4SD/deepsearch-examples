{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d96e78",
   "metadata": {},
   "source": [
    "# Document Conversion - Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb626f",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The [Deep Search Toolkit](https://ds4sd.github.io/deepsearch-toolkit/) allows document conversion with the following few lines of code. It's that simple! For more info or step-by-step guide:\n",
    "- Visit https://ds4sd.github.io/deepsearch-toolkit/guide/convert_doc/\n",
    "- Follow this example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9c441",
   "metadata": {},
   "source": [
    "### Set notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsnotebooks.settings import ProjectNotebookSettings\n",
    "\n",
    "# notebook settings auto-loaded from .env / env vars\n",
    "notebook_settings = ProjectNotebookSettings()\n",
    "\n",
    "PROFILE_NAME = notebook_settings.profile  # the profile to use\n",
    "PROJ_KEY = notebook_settings.proj_key  # the project to use\n",
    "\n",
    "# default project_key = 1234567890abcdefghijklmnopqrstvwyz123456"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dc0f1",
   "metadata": {},
   "source": [
    "### Import example dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cdef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T12:14:25.377422Z",
     "start_time": "2022-08-02T12:14:25.152485Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import deepsearch as ds\n",
    "\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from deepsearch.documents.core.export import export_to_markdown\n",
    "from IPython.display import display, Markdown, HTML, display_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4dcda",
   "metadata": {},
   "source": [
    "### Connect to Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ds.CpsApi.from_env(profile_name=PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1200c5-1138-4491-bc33-3b2d5aabe949",
   "metadata": {},
   "source": [
    "## Convert Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83eb0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T12:14:49.216045Z",
     "start_time": "2022-08-02T12:14:25.380757Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = Path(\"./converted_docs\")\n",
    "\n",
    "fname = \"20140197356.pdf\"\n",
    "\n",
    "documents = ds.convert_documents(\n",
    "    api=api,\n",
    "    proj_key=PROJ_KEY,\n",
    "    source_path=f\"../../data/samples/{fname}\",\n",
    "    progress_bar=True,\n",
    ")\n",
    "documents.download_all(result_dir=output_dir)\n",
    "info = documents.generate_report(result_dir=output_dir)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c4869-cca9-43fc-8052-c0ab7e9c175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group output files and visualize the output\n",
    "md_files = list(output_dir.glob(\"*.md\"))\n",
    "json_files = list(output_dir.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f7678-b650-484b-a994-150d0c4ec3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display last document\n",
    "# display(Markdown(md_files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784c8a9-4b96-4385-a04e-40ddbf6c613f",
   "metadata": {},
   "source": [
    "## Analyse Document with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574c77b-58a5-4259-b8e9-03ce7129c3bf",
   "metadata": {},
   "source": [
    "### term counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c995060-aee3-4b6c-ba8b-dce6288e44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_files[-1]) as fr:\n",
    "    doc = json.load(fr)\n",
    "\n",
    "terms = [\n",
    "    \"METAL\",\n",
    "    \"COPPER\",\n",
    "    \"COBALT\",\n",
    "    \"TUNGSTEN\",\n",
    "    \"MOLYBDENUM\",\n",
    "    \"RUTHENIUM\",\n",
    "    \"Self-assembly material\",\n",
    "    \"Self-assembly molecular layer\",\n",
    "    \"surface modification\",\n",
    "    \"inhibitor\",\n",
    "    \"corrosion inhibitor\",\n",
    "    \"adsorption\",\n",
    "    \"selectivity\",\n",
    "    \"Anti-corrosion\",\n",
    "    \"contact angle\",\n",
    "    \"Area selective deposition\",\n",
    "    \"Advanced interconnect metallization\",\n",
    "    \"Integrated circuits\",\n",
    "    \"Atomic layer deposition\",\n",
    "]\n",
    "\n",
    "term_hist = [{\"key\": term, \"count\": 0} for term in terms]\n",
    "\n",
    "for i, item in enumerate(doc[\"main-text\"]):\n",
    "\n",
    "    if \"text\" not in item:\n",
    "        continue\n",
    "\n",
    "    for j, term in enumerate(terms):\n",
    "        term_hist[j][\"count\"] += item[\"text\"].count(term.lower())\n",
    "\n",
    "df = pd.DataFrame(term_hist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3ff4-c444-499e-ae40-53f9833bb5ff",
   "metadata": {},
   "source": [
    "### term analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12955-dced-4320-b625-225bcb6bcdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsearch_glm.utils.load_pretrained_models import load_pretrained_nlp_models\n",
    "\n",
    "from deepsearch_glm.nlp_utils import (\n",
    "    extract_references_from_doc,\n",
    "    init_nlp_model,\n",
    "    list_nlp_model_configs,\n",
    ")\n",
    "\n",
    "from deepsearch_glm.glm_utils import (\n",
    "    create_glm_config_from_docs,\n",
    "    create_glm_dir,\n",
    "    create_glm_from_docs,\n",
    "    expand_terms,\n",
    "    load_glm,\n",
    "    read_edges_in_dataframe,\n",
    "    read_nodes_in_dataframe,\n",
    "    show_query_result,\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "models = load_pretrained_nlp_models()\n",
    "# print(f\"models: {models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3612b4-bbd2-42d0-ba2d-f8f994565380",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_files[-1]) as fr:\n",
    "    doc = json.load(fr)\n",
    "\n",
    "model = init_nlp_model(\"language;term\")\n",
    "\n",
    "for i, item in enumerate(doc[\"main-text\"]):\n",
    "\n",
    "    if \"text\" in item:\n",
    "        res = model.apply_on_text(item[\"text\"])\n",
    "        # print(res.keys())\n",
    "\n",
    "        # print(item[\"text\"])\n",
    "        # print(tabulate(res[\"instances\"][\"data\"],\n",
    "        #               headers=res[\"instances\"][\"headers\"]))\n",
    "\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "\n",
    "res = model.apply_on_doc(doc)\n",
    "\n",
    "df = pd.DataFrame(res[\"instances\"][\"data\"], columns=res[\"instances\"][\"headers\"])\n",
    "\n",
    "terms = df[df[\"type\"] == \"term\"][[\"type\", \"name\", \"subj_path\"]]\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665335e2-4a10-44ca-af15-0e4b9f8d031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "odir = \"./glm\"\n",
    "os.makedirs(odir, exist_ok=True)\n",
    "\n",
    "model_names = \"spm;term\"\n",
    "#json_files = [json_files[-1]]\n",
    "\n",
    "odir, glm = create_glm_from_docs(odir, json_files, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175550c9-8b1e-4b02-8069-6c36aceafd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = read_nodes_in_dataframe(\"./glm/nodes.csv\")\n",
    "# print(nodes)\n",
    "\n",
    "# Get all terms of the document\n",
    "terms = nodes[nodes[\"name\"] == \"term\"][[\"total-count\", \"nodes-text\"]]\n",
    "print(terms)\n",
    "\n",
    "# Get all terms of the document with `composition`\n",
    "res = expand_terms(glm, \"composition\")\n",
    "# show_query_result(res)\n",
    "\n",
    "last_result = res[\"result\"][-1][\"nodes\"]\n",
    "expanded_terms = pd.DataFrame(last_result[\"data\"], columns=last_result[\"headers\"])\n",
    "\n",
    "expanded_terms = expanded_terms[[\"weight\", \"prob\", \"cumul\", \"text\", \"count\"]]\n",
    "print(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01771757-70c3-44cb-824c-1fd9b716a99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
